defaults:
  learning_rate: 1e-5
  gradient_checkpointing: false
  gradient_accumulation_steps: 32
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  weight_decay: 0.00
  warmup_steps: 600
  eval_steps: 500
  save_steps: 500
  max_length: 512
  num_train_epochs: 3
  logging_steps: 10
  max_grad_norm: 2.0
  save_total_limit: 4
  eval_accumulation_steps:
  freeze_layer:
  datasets:
    - webgpt: 1
    - prompt_dialogue: 1
    - squad_v2: 1
    - adversarial_qa: 1
    - trivia_qa_nocontext: 1
    - xsum: 1
    - cnn_dailymail: 1
    - prompt_dialogue: 1
    - multi_news: 1
    - scitldr: 1
    - soda: 1
    - joke: 1
    - gsm8k: 1
    - dive_mt: 1
    - wmt2019_zh-en: 1
    - wmt2019_ru-en: 1
    - wmt2019_de-en: 1
    - ted_trans_nl-en: 1
    - ted_trans_de-ja: 1
    - instruct_tuning: 1
    - wmt2019_de-en: 1
    - samsum: 1
    - soda_dialogue: 1
  cache_dir: .cache
  loss_fn: CrossEntropyLoss
  eval_size: 25
  log_dir: "base"
  quantization: false
  seq2seqmodel: false
  poly_eps: 1.0

galactica-125m:
  learning_rate: 5e-5
  model_name: facebook/galactica-125m
  weight_decay: 0.01
  warmup_steps: 600
  gradient_checkpointing: false
  gradient_accumulation_steps: 2
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4

gpt-jt:
  learning_rate: 2e-6
  model_name: togethercomputer/GPT-JT-6B-v1
  weight_decay: 0.01
  max_length: 1024
  warmup_steps: 600
  gradient_checkpointing: false
  gradient_accumulation_steps: 2
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4

codegen:
  learning_rate: 8e-6
  model_name: Salesforce/codegen-2B-multi
  weight_decay: 0.01
  max_length: 520
  warmup_steps: 1000
  gradient_checkpointing: false
  gradient_accumulation_steps: 9
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4

debug:
  eval_steps: 20
  eval_size: 20
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  quantization: false
